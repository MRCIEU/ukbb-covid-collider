---
title: UK Biobank example of collider bias in Covid-19 test data
bibliography: library.bib
output:
  pdf_document:
    toc: true
  html_document:
    toc: true
    toc_float: true
---

## About this document

This document forms part of the analysis used in the paper:

**Collider bias undermines our understanding of COVID-19 disease risk and severity**. Gareth Griffith, Tim T Morris, Matt Tudball, Annie Herbert, Giulia Mancano, Lindsey Pike, Gemma C Sharp, Tom M Palmer, George Davey Smith, Kate Tilling, Luisa Zuccolo, Neil M Davies, Gibran Hemani

It is hosted at https://github.com/MRCIEU/ukbb-covid-collider.

Here we show a set of analyses to illustrate collider bias induced by non-random testing of Covid-19 status amongst the UK Biobank participants, and some approaches to adjust for the bias. The methods are described in further detail in @Griffith2020.05.04.20090506.

The following variables from the [UK biobank phenotype data](http://biobank.ctsu.ox.ac.uk/crystal/) are used:

- `34-0.0` - Year of birth (converted into age for this analysis)
- `31-0.0` - Sex (male = 1, female = 0)
- `23104-0.0` - Body mass index (BMI)

Also, the linked Covid-19 freeze from `2020-06-05` is used to identify which individuals have been tested and tested positive.

In the analysis that follows, we will be estimating the association between testing positive for Covid-19 and the risk factors age, sex and BMI. The key concern with such an analysis is that we only observe test results among individuals who have received a test. SARS-CoV-2 infection and the risk factors themselves will influence the likelihood of receiving a test, which could induce spurious associations among them when we condition on receiving a test. We will explore inverse probability weighting and sensitivity analyses to address the potential collider bias.

## Read in the data

```{r}
suppressMessages(suppressPackageStartupMessages({
  library(knitr)
  library(dplyr)
  library(ggplot2)
  library(selectioninterval)
}))

knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=TRUE, cache=TRUE)
```

```{r}
load("data/dat.rdata")
dat <- dat[complete.cases(dat[,c("age","sex","bmi","tested")]), ]
str(dat)
```

Summaries:

```{r}
hist(dat$age)
table(dat$sex) / nrow(dat)
```

How many individuals tested:

```{r}
table(dat$tested)
```

How many individuals tested positive:

```{r}
table(dat$positive)
```

## Unweighted associations

The most basic approach is to report the raw pairwise associations between the risk factors and testing positive. 

```{r}
assoc <- data.frame(matrix(nrow=3, ncol=3))
names(assoc) <- c("risk_factor", "beta", "se")
assoc$risk_factor <- c("sex", "age", "bmi")

mod <- summary(lm(positive ~ sex, dat))
assoc[assoc$risk_factor=="sex",c("beta","se")] <-  coefficients(mod)["sex",c("Estimate","Std. Error")]

mod <- summary(lm(positive ~ age, dat))
assoc[assoc$risk_factor=="age",c("beta","se")] <-  coefficients(mod)["age",c("Estimate","Std. Error")]

mod <- summary(lm(positive ~ bmi, dat))
assoc[assoc$risk_factor=="bmi",c("beta","se")] <-  coefficients(mod)["bmi",c("Estimate","Std. Error")]

ggplot(assoc, aes(risk_factor, beta)) + 
  geom_errorbar(
    aes(ymin = beta-1.96*se, 
        ymax = beta+1.96*se),
    width = 0.1, size=1, color="#00BFC4") + 
geom_point(size=3, shape=21, fill="#00BFC4") +
geom_hline(yintercept=0, linetype="dashed", color = "#575863") + 
scale_x_discrete(labels=c("Age", "BMI", "Sex")) +
xlab("") + ylab("Value of association") + 
coord_flip()
```

However, it is unclear to what extent these associations may be distorted by collider bias.

## Illustration of collider bias

An simple exploratory exercise is to compare full sample and sub-sample associations among the variables that are observed in both, namely, age, sex and BMI. 

We begin by exploring their relationship with receiving a test.

```{r}
summary(glm(tested ~ age + sex + bmi, data=dat, family="binomial"))
```

Therefore, we would expect the relationships between these variables to be skewed within the tested sample. For example, this is the relationship between age and sex in the full sample:

```{r}
mod1 <- summary(lm(age ~ sex, dat))
coefficients(mod1) %>% kable
```

And here it is among those who received a test:

```{r}
mod2 <- summary(lm(age ~ sex, dat, subset=dat$tested==1))
coefficients(mod2) %>% kable
```

Note that the association is around **`r round(coefficients(mod2)[2,1] / coefficients(mod1)[2,1])` times larger** in the tested subset.

Similarly, this is the overall association between age and BMI:

```{r}
mod3 <- summary(lm(bmi ~ sex, dat))
coefficients(mod3) %>% kable
```

and here in the subsample:

```{r}
mod4 <- summary(lm(bmi ~ sex, dat, subset=dat$tested==1))
coefficients(mod4) %>% kable
```

where the association almost halves.

## Inverse probability weights in a nested sample

The first approach we consider is inverse probability weighting. Since tested individuals are a subset of the entire sample, it is possible to generate probabilities of being included in the tested sub-sample based on the measured risk factors. The inverse of these probabilities can be used to weight the association estimate towards being representative of the full sample. 

Note that, since testing positive is only measured within the tested sub-sample, we cannot include it in the weighting model. Therefore, for the inverse weighted estimates to be unbiased for the full-sample risk factor associations, we would need each individual's likelihood of being tested to be independent of whether they are infected with Covid-19 given their age, sex and BMI. Since this is unlikely to be true, we will explore sensitivity analyses later in the tutorial.

First, find which variables associate with being tested. Begin with marginal effects

```{r}
mod5 <- glm(tested ~ age + sex + bmi, data=dat, family="binomial")
prs1 <- fitted.values(mod5)
coefficients(summary(mod5)) %>% kable
```

It's important to also test for interactions between variables [@Groenwold2020].

```{r}
mod6 <- glm(tested ~ bmi + age + sex + bmi * age + bmi * sex + age * sex + age * bmi * sex, data=dat, family="binomial")
coefficients(summary(mod6)) %>% kable
```

Curiously, the marginal BMI effect appears to be captured by interaction terms only. Centering the marginal variables may be more appropriate when estimating interactions, as otherwise the interaction term is simply a multiplicative effect which is correlated to the marginal effects.

```{r}
dat$bmixage <- scale(dat$bmi) * scale(dat$age)
dat$bmixsex <- scale(dat$bmi) * scale(dat$sex)
dat$agexsex <- scale(dat$age) * scale(dat$sex)
dat$agexsexxbmi <- scale(dat$age) * scale(dat$sex) * scale(dat$bmi)
mod7 <- glm(tested ~ bmi + age + sex + bmixage + bmixsex + agexsex + agexsexxbmi, data=dat, family="binomial")
coefficients(summary(mod7)) %>% kable
```

Here the marginal effects are all retained.

We can generate the probabilities of each individual being tested based on these variables using:

```{r plot_prs2}
prs2 <- fitted.values(mod7)
hist(prs2, breaks=100)
```

It is also important to consider non-linearities in the relationship between testing and age and BMI, which are continuous. There appears to be a quadratic relationship between age and testing and a cubic relationship between BMI and testing. 

```{r nonlinear_plots}
# Age plot
ggplot(dat[dat$age<82,],aes(x=age, y=tested)) +
  geom_smooth(method="gam",formula=y~s(x)) +
  xlab("Age") + ylab("Likelihood of Testing") + 
  scale_x_continuous(breaks=seq(min(dat$age),82,by=2))

# BMI plot
ggplot(dat[dat$bmi<60,],aes(x=bmi, y=tested)) +
   geom_smooth(method="gam",formula=y~s(x)) +
  xlab("BMI") + ylab("Likelihood of Testing") + 
  scale_x_continuous(breaks=seq(12,60,by=5))
```

We select a logistic regression which is quadratic in age, cubic in BMI and contains all linear interactions between age, sex and BMI.

```{r model_nonlinear}
mod8 <- glm(tested ~ sex + poly(age, 2) + poly(bmi, 3) + bmixage + bmixsex + agexsex + agexsexxbmi, data=dat, family="binomial")
summary(mod8)
prs3 <- fitted.values(mod8)
hist(prs3, breaks=100)
```

The probabilities are largely clustered close to zero, so when using the inverse of these probabilities for weights there can be potential instability that comes from dividing by small numbers. Stabilising the weight, by multiplying by the probability of being tested, can avoid this issue [@Sayon-Orea2020].

```{r stabilise}
p <- mean(as.numeric(dat$tested)) 
ipw1 <- ifelse(dat$tested==1, p/prs1, (1-p)/(1-prs1))
ipw2 <- ifelse(dat$tested==1, p/prs2, (1-p)/(1-prs2))
ipw3 <- ifelse(dat$tested==1, p/prs3, (1-p)/(1-prs3))
```

### Validation exercise: Comparing full sample, unweighted and weighted associations among fully measured variables

Below we perform a validation exercise where we compare the full sample associations with the weighted associations for age, sex and BMI.

```{r models}
# Prepare output file
group <- expand.grid(c("as","ab","bs"),c(1,2,3,4,5))
out <- data.frame(cbind(group, matrix(nrow=15, ncol=2)))
names(out) <- c("assoc","model","beta","se")

# Age and sex association
# Full sample
mod <- summary(lm(age ~ sex, dat))
out[out$assoc=="as"&out$model==5,c("beta","se")] <- coefficients(mod)["sex",c("Estimate","Std. Error")]

# Tested subsample
mod <- summary(lm(age ~ sex, dat, subset=dat$tested==1))
out[out$assoc=="as"&out$model==4,c("beta","se")] <- coefficients(mod)["sex",c("Estimate","Std. Error")]

# Inverse weighted (linear, no interactions)
mod <- summary(lm(age ~ sex, dat, weight=ipw1, subset=tested==1))
out[out$assoc=="as"&out$model==1,c("beta","se")] <- coefficients(mod)["sex",c("Estimate","Std. Error")]

# Inverse weighted (linear, interactions)
mod <- summary(lm(age ~ sex, dat, weight=ipw2, subset=tested==1))
out[out$assoc=="as"&out$model==2,c("beta","se")] <- coefficients(mod)["sex",c("Estimate","Std. Error")]

# Inverse weighted (nonlinear, interactions)
mod <- summary(lm(age ~ sex, dat, weight=ipw3, subset=tested==1))
out[out$assoc=="as"&out$model==3,c("beta","se")] <- coefficients(mod)["sex",c("Estimate","Std. Error")]

# BMI and sex association
# Full sample
mod <- summary(lm(bmi ~ sex, dat))
out[out$assoc=="bs"&out$model==5,c("beta","se")] <- coefficients(mod)["sex",c("Estimate","Std. Error")]

# Tested subsample
mod <- summary(lm(bmi ~ sex, dat, subset=dat$tested==1))
out[out$assoc=="bs"&out$model==4,c("beta","se")] <- coefficients(mod)["sex",c("Estimate","Std. Error")]

# Inverse weighted (linear, no interactions)
mod <- summary(lm(bmi ~ sex, dat, weight=ipw1, subset=tested==1))
out[out$assoc=="bs"&out$model==1,c("beta","se")] <- coefficients(mod)["sex",c("Estimate","Std. Error")]

# Inverse weighted (linear, interactions)
mod <- summary(lm(bmi ~ sex, dat, weight=ipw2, subset=tested==1))
out[out$assoc=="bs"&out$model==2,c("beta","se")] <- coefficients(mod)["sex",c("Estimate","Std. Error")]

# Inverse weighted (nonlinear, interactions)
mod <- summary(lm(bmi ~ sex, dat, weight=ipw3, subset=tested==1))
out[out$assoc=="bs"&out$model==3,c("beta","se")] <- coefficients(mod)["sex",c("Estimate","Std. Error")]

# BMI and age association
# Full sample
mod <- summary(lm(bmi ~ age, dat))
out[out$assoc=="ab"&out$model==5,c("beta","se")] <- coefficients(mod)["age",c("Estimate","Std. Error")]

# Tested subsample
mod <- summary(lm(bmi ~ age, dat, subset=dat$tested==1))
out[out$assoc=="ab"&out$model==4,c("beta","se")] <- coefficients(mod)["age",c("Estimate","Std. Error")]

# Inverse weighted (linear, no interactions)
mod <- summary(lm(bmi ~ age, dat, weight=ipw1, subset=tested==1))
out[out$assoc=="ab"&out$model==1,c("beta","se")] <- coefficients(mod)["age",c("Estimate","Std. Error")]

# Inverse weighted (linear, interactions)
mod <- summary(lm(bmi ~ age, dat, weight=ipw2, subset=tested==1))
out[out$assoc=="ab"&out$model==2,c("beta","se")] <- coefficients(mod)["age",c("Estimate","Std. Error")]

# Inverse weighted (nonlinear, interactions)
mod <- summary(lm(bmi ~ age, dat, weight=ipw3, subset=tested==1))
out[out$assoc=="ab"&out$model==3,c("beta","se")] <- coefficients(mod)["age",c("Estimate","Std. Error")]
```

#### Age and sex association

```{r age_sex}
ggplot() + geom_errorbar(data=out[out$assoc=="as",], mapping=aes(x=as.factor(model), ymin=beta-1.96*se,    ymax=beta+1.96*se), width=0.1, size=1, color="#00BFC4") + 
geom_point(data=out[out$assoc=="as",], mapping=aes(x=as.factor(model), y=beta), size=3, shape=21, fill="#00BFC4") + scale_x_discrete(labels=c("Linear / no interactions", "Linear / interactions",  "Nonlinear / interactions", "Unweighted", "Full sample")) + xlab("") + ylab("Value of association")
```

#### BMI and sex association

```{r bmi_sex}
ggplot() + geom_errorbar(data=out[out$assoc=="bs",], mapping=aes(x=as.factor(model), ymin=beta-1.96*se,    ymax=beta+1.96*se), width=0.1, size=1, color="#00BFC4") + 
geom_point(data=out[out$assoc=="bs",], mapping=aes(x=as.factor(model), y=beta), size=3, shape=21, fill="#00BFC4") + scale_x_discrete(labels=c("Linear / no interactions", "Linear / interactions",  "Nonlinear / interactions", "Unweighted", "Full sample")) + xlab("") + ylab("Value of association")
```

#### BMI and age association

```{r bmi_age}
ggplot() + geom_errorbar(data=out[out$assoc=="ab",], mapping=aes(x=as.factor(model), ymin=beta-1.96*se,    ymax=beta+1.96*se), width=0.1, size=1, color="#00BFC4") + 
geom_point(data=out[out$assoc=="ab",], mapping=aes(x=as.factor(model), y=beta), size=3, shape=21, fill="#00BFC4") + scale_x_discrete(labels=c("Linear / no interactions", "Linear / interactions",  "Nonlinear / interactions", "Unweighted", "Full sample")) + xlab("") + ylab("Value of association")
```

## Comparing weighted and unweighted associations between risk factors and testing positive

The previous validation exercise suggests that the full weighting model with interactions and non-linearities performs best at recovering the full sample associations among sex, age and BMI. While this does not mean it will necessarily recover the full sample associations between risk factors and testing positive, it is a useful starting point. Below we compare the weighted risk factor associations with the unweighted associations estimated earlier.

```{r}
assoc2 <- data.frame(matrix(nrow=3, ncol=3))
names(assoc2) <- c("risk_factor", "beta", "se")
assoc2$risk_factor <- c("sex", "age", "bmi")

mod <- summary(lm(positive ~ sex, dat, weight=ipw3))
assoc2[assoc2$risk_factor=="sex",c("beta","se")] <-  coefficients(mod)["sex",c("Estimate","Std. Error")]

mod <- summary(lm(positive ~ age, dat, weight=ipw3))
assoc2[assoc2$risk_factor=="age",c("beta","se")] <-  coefficients(mod)["age",c("Estimate","Std. Error")]

mod <- summary(lm(positive ~ bmi, dat, weight=ipw3))
assoc2[assoc2$risk_factor=="bmi",c("beta","se")] <-  coefficients(mod)["bmi",c("Estimate","Std. Error")]

assoc <- rbind(assoc, assoc2)
assoc$model <- c(rep(1,3),rep(2,3))

ggplot(assoc, aes(risk_factor, beta)) +
  geom_errorbar(
    aes(ymin = beta-1.96*se, 
        ymax = beta+1.96*se, 
        color = as.factor(model)),
    position = position_dodge(0.3), size=1, width=0.1
  ) +
  xlab("") + ylab("Value of association") +
  geom_point(aes(color = as.factor(model)), size=3, position = position_dodge(0.3)) +
  geom_hline(yintercept=0, linetype="dashed", color = "#575863") +
  labs(colour="") + 
  scale_color_manual(name="Estimates", labels=c("Unweighted", "Weighted"), values=c("#F8766D","#00BFC4")) +
  coord_flip()
```

The weighted and unweighted associations are very similar. This could be indicative of no collider bias, however, it is more likely in this instance that the weights are misspecified due to the absence of the positive test variable. Below we explore sensitivity analyses for this misspecification.

## Sensitivity analyses for non-nested samples

We now present a sensitivity analysis which can be used when direct estimation of the probability weights is limited or not possible. @Tudball2021 is most appropriately used when weights cannot be estimated but there is external information on the target population (e.g. survey response rate, population means). We consider the example of the association between BMI and test positivity.

### @Tudball2021

Suppose we only observe the tested sub-sample. Estimating probability weights is no longer possible since we do not observe anyone who has not been tested. @Tudball2021 allows us to propose a logistic model for the probability weights, comprised of variables measured in the tested sub-sample. There are three sensitivity parameters: 1) a lower bound for the average probability of sample selection; 2) an upper bound for the average probability of sample selection; and 3) the change in odds of sample selection for a one standard deviation increase in each variable.

An advantage of this method is that we can place additional constraints on the bounds. Suppose we know the unconditional likelihood of being tested and the average age and BMI of UK Biobank participants. We can include these as constraints to ensure that the resulting bounds are consistent with these population values. In reality, we may have more or fewer constraints than this.

```{r tudball_data}
dattest <- dat[complete.cases(dat), ]
dattest$bmi <- ifelse(dattest$bmi > 45, 45, dattest$bmi)

w <- as.data.frame(dattest[,(colnames(dattest) %in% c('age','sex','bmi','positive'))])
w$age2 <- w$age^2
w$bmi2 <- w$bmi^2
w$int <- w$bmi*w$positive
```

We then define our sensitivity parameters. These imply that the average individual in the sample has between a 1\% and 50\% probability of sample selection. Furthermore, a standard deviation increase in each variable in the weight model can change the odds of sample selection between 1/3 and 3.

```{r tudball_bound}
p <- c(0.01, 0.5, 3)
```

We also select our additional constraints. We choose the response rate of the tested sub-sample, as well as the average age, sex and BMI in the full UK Biobank sample. We also ensure that being positive for Covid-19 increases one's likelihood of being tested.

```{r}
mycons <- list(
  list('RESP', mean(dat$tested)),
  list('COVMEAN', w$age, mean(dat$age)),
  list('COVMEAN', w$bmi, mean(dat$bmi)),
  list('COVMEAN', w$sex, mean(dat$sex)),
  list('DIREC', 4, '+')
)
```

We are now ready to generate our bounds for the association between BMI and testing positive. To do this we will use the package `selectioninterval` available at https://github.com/matt-tudball/selectioninterval.

```{r tudball_calculate}
out <- selectioninterval::selection_bound(
        y=dattest$positive, 
        x=dattest$bmi, 
        w=w, 
        L0l=p[1], 
        L0u=p[2], 
        L1=p[3], 
        cons=mycons)

tibble(
  method="Tudball et al (2021)", 
  lower_ci=round(out$ci[1],3), 
  lower_bound=round(out$interval[1],3),
  upper_bound=round(out$interval[2],3),
  upper_ci=round(out$ci[2],3)
  ) %>% kable
```

## Summary

Here we have walked through a realistic example of estimating risk factors for testing positive for Covid-19 in UK Biobank. As a validation exercise, we have shown that inverse probability weighting is able to recover sample-wide associations between BMI, age and sex within the non-random sub-sample of individuals who have been tested for Covid-19. We also show that if the data is non-nested, in that a model cannot be fitted to predict participation, then summary data from a reference population can be used to generate adjusted estimates within the selected sample.

In general practice, there are a few things to further consider. 

- We only used BMI, age and sex to develop weights here. In principle one would actually use as many variables as is necessary (even if they are not the exact variables being analysed) to build the model for sample selection.
- There is likely to be a change in selection pressure over time, so it is important to use weights that are relevant to the timings of the variables being recorded
- Third, in the UK Biobank the reality is slightly more complicated than has been presented here, in that the samples in the overall UK Biobank are themselves non-random (and not representative of the general population) [@Munafo2018-fv]. 

## References