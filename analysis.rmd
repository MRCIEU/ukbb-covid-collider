---
title: UK Biobank example of collider bias in Covid-19 test data
bibliography: library.bib
output:
  html_document:
    toc: true
    toc_float: true
---

## About this document

This document forms part of the analysis used in the paper:

**Collider bias undermines our understanding of COVID-19 disease risk and severity**. Gareth Griffith, Tim T Morris, Matt Tudball, Annie Herbert, Giulia Mancano, Lindsey Pike, Gemma C Sharp, Tom M Palmer, George Davey Smith, Kate Tilling, Luisa Zuccolo, Neil M Davies, Gibran Hemani

It is hosted at https://github.com/MRCIEU/ukbb-covid-collider.

Here we show a set of analyses to illustrate collider bias induced by non-random testing of Covid-19 status amongst the UK Biobank participants, and some approaches to adjust for the bias. The methods are described in further detail in @Griffith2020.05.04.20090506.

The following variables from the [UK biobank phenotype data](http://biobank.ctsu.ox.ac.uk/crystal/) are used:

- `34-0.0` - Year of birth (converted into age for this analysis)
- `31-0.0` - Sex (male = 1, female = 0)
- `23104-0.0` - Body mass index (BMI)

Also, the linked Covid-19 freeze from `2020-06-05` is used to identify which individuals have been tested.

The analysis that follows will look at simple associations between age, sex and BMI in the entire UK Biobank sample, contrasted against their associations amongst individuals selected to be tested.


## Read in the data

```{r}
suppressMessages(suppressPackageStartupMessages({
  library(knitr)
  library(dplyr)
  library(ggplot2)
  library(bootsens)
}))

knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=TRUE, cache=TRUE)
```

```{r}
load("data/dat.rdata")
dat <- dat[complete.cases(dat), ]
str(dat)
```

Summaries:

```{r}
hist(dat$age)
table(dat$sex) / nrow(dat)
```

How many individuals tested:

```{r}
table(dat$tested)
```

## Illustration of collider bias

Age, sex and BMI all strongly influence the likelihood of being tested:

```{r}
summary(glm(tested ~ age + sex + bmi, data=dat, family="binomial"))
```

Hence we would expect the relationships between these variables to be skewed within the tested sample. For example, this is the relationship between age and sex in the total sample

```{r}
mod1 <- summary(lm(age ~ sex, dat))
coefficients(mod1) %>% kable
```

And here it is amongst only those tested:

```{r}
mod2 <- summary(lm(age ~ sex, dat, subset=dat$tested==1))
coefficients(mod2) %>% kable
```

Note that the association is around **`r round(coefficients(mod2)[2,1] / coefficients(mod1)[2,1])` times larger** in the tested subset.

Similarly, this is the overall association between age and BMI:

```{r}
mod3 <- summary(lm(bmi ~ sex, dat))
coefficients(mod3) %>% kable
```

and here in the subsample:

```{r}
mod4 <- summary(lm(bmi ~ sex, dat, subset=dat$tested==1))
coefficients(mod4) %>% kable
```

where the association almost halves.

## Inverse probability weights in a nested sample

If the tested individuals are a subset of the entire sample, then it is possible to generate probabilities for individuals being included in the tested subsample based on the colliding variables. The inverse of these probabilities can be used to weight the association estimate towards being representative of the overall sample.

First find which variables associate with being tested. Begin with marginal effects

```{r}
mod5 <- glm(tested ~ age + sex + bmi, data=dat, family="binomial")
prs1 <- fitted.values(mod5)
coefficients(summary(mod5)) %>% kable
```

It's important to also test for interactions between variables [@Groenwold2020].

```{r}
mod6 <- glm(tested ~ bmi + age + sex + bmi * age + bmi * sex + age * sex + age * bmi * sex, data=dat, family="binomial")
coefficients(summary(mod6)) %>% kable
```

Curiously the marginal BMI effect appears to be captured by interaction terms only. Centering the marginal variables may be more appropriate when estimating interactions.

```{r}
dat$bmixage <- scale(dat$bmi) * scale(dat$age)
dat$bmixsex <- scale(dat$bmi) * scale(dat$sex)
dat$agexsex <- scale(dat$age) * scale(dat$sex)
dat$agexsexxbmi <- scale(dat$age) * scale(dat$sex) * scale(dat$bmi)
mod7 <- glm(tested ~ bmi + age + sex + bmixage + bmixsex + agexsex + agexsexxbmi, data=dat, family="binomial")
coefficients(summary(mod7)) %>% kable
```

Here the marginal effects are all retained.

We can generate the probabilities of each individual being tested based on these variables using:

```{r plot_prs2}
prs2 <- fitted.values(mod7)
hist(prs2, breaks=100)
```

It is also important to consider non-linearities in the relationship between testing and age and BMI, which are continuous. There appears to be a quadratic relationship between age and testing and a cubic relationship between BMI and testing. 

```{r nonlinear_plots}
# Age plot
ggplot(dat[dat$age<82,],aes(x=age, y=tested)) +
  geom_smooth(method="gam",formula=y~s(x)) +
  xlab("Age") + ylab("Likelihood of Testing") + 
  scale_x_continuous(breaks=seq(min(dat$age),82,by=2))

# BMI plot
ggplot(dat[dat$bmi<60,],aes(x=bmi, y=tested)) +
   geom_smooth(method="gam",formula=y~s(x)) +
  xlab("BMI") + ylab("Likelihood of Testing") + 
  scale_x_continuous(breaks=seq(12,60,by=5))
```

We select a linear regression which is quadratic in age, cubic in BMI and contains all linear interactions between age, sex and BMI. Results are similar for logistic regression.

```{r model_nonlinear}
mod8 <- glm(tested ~ sex + poly(age, 2) + poly(bmi, 3) + bmixage + bmixsex + agexsex + agexsexxbmi, data=dat, family="binomial")
summary(mod8)
prs3 <- fitted.values(mod8)
hist(prs3, breaks=100)
```

The probabilities are largely clustered close to zero, so when using the inverse of these probabilities for weights there can be potential instability that comes from dividing by small numbers. Stabilising the weight, by multiplying by the probability of being tested, can avoid this issue [@Sayon-Orea2020].

```{r stabilise}
p <- mean(as.numeric(dat$tested)) 
ipw1 <- ifelse(dat$tested==1, p/prs1, (1-p)/prs1)
ipw2 <- ifelse(dat$tested==1, p/prs2, (1-p)/prs2)
ipw3 <- ifelse(dat$tested==1, p/prs3, (1-p)/prs3)
```

## Comparison of full sample, unweighted and weighted estimates

Below we generate the estimates in the full sample and in the tested subsample using different weights.

```{r models}
# Prepare output file
group <- expand.grid(c("as","ab","bs"),c(1,2,3,4,5))
out <- data.frame(cbind(group, matrix(nrow=15, ncol=2)))
names(out) <- c("assoc","model","beta","se")

# Age and sex association
# Full sample
mod1 <- summary(lm(age ~ sex, dat))
out[out$assoc=="as"&out$model==5,"beta"] <- coefficients(mod1)["sex","Estimate"]
out[out$assoc=="as"&out$model==5,"se"] <- coefficients(mod1)["sex","Std. Error"]
# Tested subsample
mod2 <- summary(lm(age ~ sex, dat, subset=dat$tested==1))
out[out$assoc=="as"&out$model==4,"beta"] <- coefficients(mod2)["sex","Estimate"]
out[out$assoc=="as"&out$model==4,"se"] <- coefficients(mod2)["sex","Std. Error"]

# Inverse weighted (linear, no interactions)
mod3 <- summary(lm(age ~ sex, dat, weight=ipw1, subset=tested==1))
out[out$assoc=="as"&out$model==1,"beta"] <- coefficients(mod3)["sex","Estimate"]
out[out$assoc=="as"&out$model==1,"se"] <- coefficients(mod3)["sex","Std. Error"]

# Inverse weighted (linear, interactions)
mod4 <- summary(lm(age ~ sex, dat, weight=ipw2, subset=tested==1))
out[out$assoc=="as"&out$model==2,"beta"] <- coefficients(mod4)["sex","Estimate"]
out[out$assoc=="as"&out$model==2,"se"] <- coefficients(mod4)["sex","Std. Error"]

# Inverse weighted (nonlinear, interactions)
mod5 <- summary(lm(age ~ sex, dat, weight=ipw3, subset=tested==1))
out[out$assoc=="as"&out$model==3,"beta"] <- coefficients(mod5)["sex","Estimate"]
out[out$assoc=="as"&out$model==3,"se"] <- coefficients(mod5)["sex","Std. Error"]

# BMI and sex association
# Full sample
mod1 <- summary(lm(bmi ~ sex, dat))
out[out$assoc=="bs"&out$model==5,"beta"] <- coefficients(mod1)["sex","Estimate"]
out[out$assoc=="bs"&out$model==5,"se"] <- coefficients(mod1)["sex","Std. Error"]
# Tested subsample
mod2 <- summary(lm(bmi ~ sex, dat, subset=dat$tested==1))
out[out$assoc=="bs"&out$model==4,"beta"] <- coefficients(mod2)["sex","Estimate"]
out[out$assoc=="bs"&out$model==4,"se"] <- coefficients(mod2)["sex","Std. Error"]

# Inverse weighted (linear, no interactions)
mod3 <- summary(lm(bmi ~ sex, dat, weight=ipw1, subset=tested==1))
out[out$assoc=="bs"&out$model==1,"beta"] <- coefficients(mod3)["sex","Estimate"]
out[out$assoc=="bs"&out$model==1,"se"] <- coefficients(mod3)["sex","Std. Error"]

# Inverse weighted (linear, interactions)
mod4 <- summary(lm(bmi ~ sex, dat, weight=ipw2, subset=tested==1))
out[out$assoc=="bs"&out$model==2,"beta"] <- coefficients(mod4)["sex","Estimate"]
out[out$assoc=="bs"&out$model==2,"se"] <- coefficients(mod4)["sex","Std. Error"]

# Inverse weighted (nonlinear, interactions)
mod5 <- summary(lm(bmi ~ sex, dat, weight=ipw3, subset=tested==1))
out[out$assoc=="bs"&out$model==3,"beta"] <- coefficients(mod5)["sex","Estimate"]
out[out$assoc=="bs"&out$model==3,"se"] <- coefficients(mod5)["sex","Std. Error"]

# BMI and age association
# Full sample
mod1 <- summary(lm(bmi ~ age, dat))
out[out$assoc=="ab"&out$model==5,"beta"] <- coefficients(mod1)["age","Estimate"]
out[out$assoc=="ab"&out$model==5,"se"] <- coefficients(mod1)["age","Std. Error"]
# Tested subsample
mod2 <- summary(lm(bmi ~ age, dat, subset=dat$tested==1))
out[out$assoc=="ab"&out$model==4,"beta"] <- coefficients(mod2)["age","Estimate"]
out[out$assoc=="ab"&out$model==4,"se"] <- coefficients(mod2)["age","Std. Error"]

# Inverse weighted (linear, no interactions)
mod3 <- summary(lm(bmi ~ age, dat, weight=ipw1, subset=tested==1))
out[out$assoc=="ab"&out$model==1,"beta"] <- coefficients(mod3)["age","Estimate"]
out[out$assoc=="ab"&out$model==1,"se"] <- coefficients(mod3)["age","Std. Error"]

# Inverse weighted (linear, interactions)
mod4 <- summary(lm(bmi ~ age, dat, weight=ipw2, subset=tested==1))
out[out$assoc=="ab"&out$model==2,"beta"] <- coefficients(mod4)["age","Estimate"]
out[out$assoc=="ab"&out$model==2,"se"] <- coefficients(mod4)["age","Std. Error"]

# Inverse weighted (nonlinear, interactions)
mod5 <- summary(lm(bmi ~ age, dat, weight=ipw3, subset=tested==1))
out[out$assoc=="ab"&out$model==3,"beta"] <- coefficients(mod5)["age","Estimate"]
out[out$assoc=="ab"&out$model==3,"se"] <- coefficients(mod5)["age","Std. Error"]
```

### Age and sex association

```{r age_sex}
ggplot() + geom_errorbar(data=out[out$assoc=="as",], mapping=aes(x=as.factor(model), ymin=beta-1.96*se,    ymax=beta+1.96*se), width=0.1, size=1, color="#00BFC4") + 
geom_point(data=out[out$assoc=="as",], mapping=aes(x=as.factor(model), y=beta), size=3, shape=21, fill="#00BFC4") + scale_x_discrete(labels=c("Linear / no interactions", "Linear / interactions",  "Nonlinear / interactions", "Unweighted", "Full sample")) + xlab("") + ylab("Value of association")
```

### BMI and sex association

```{r bmi_sex}
ggplot() + geom_errorbar(data=out[out$assoc=="bs",], mapping=aes(x=as.factor(model), ymin=beta-1.96*se,    ymax=beta+1.96*se), width=0.1, size=1, color="#00BFC4") + 
geom_point(data=out[out$assoc=="bs",], mapping=aes(x=as.factor(model), y=beta), size=3, shape=21, fill="#00BFC4") + scale_x_discrete(labels=c("Linear / no interactions", "Linear / interactions",  "Nonlinear / interactions", "Unweighted", "Full sample")) + xlab("") + ylab("Value of association")
```

### BMI and age association

```{r bmi_age}
ggplot() + geom_errorbar(data=out[out$assoc=="ab",], mapping=aes(x=as.factor(model), ymin=beta-1.96*se,    ymax=beta+1.96*se), width=0.1, size=1, color="#00BFC4") + 
geom_point(data=out[out$assoc=="ab",], mapping=aes(x=as.factor(model), y=beta), size=3, shape=21, fill="#00BFC4") + scale_x_discrete(labels=c("Linear / no interactions", "Linear / interactions",  "Nonlinear / interactions", "Unweighted", "Full sample")) + xlab("") + ylab("Value of association")
```


## Sensitivity analyses for non-nested samples

We now present two sensitivity analyses which can be used when direct estimation of the probability weights is limited or not possible. Zhao et al (2019) is most appropriately used when probability weights can be estimated but are likely to be misspecified (e.g. missing an important predictor). Tudball et al (2020) is most appropriately used when weights cannot be estimated but there is external information on the target population (e.g. survey response rate, population means). 

We will attempt to estimate average BMI in the full UK Biobank population using data on the tested sub-sample. The tested sub-sample's average BMI is around 1 unit higher than the full sample.

```{r bmi_mean}
dattest <- dat[dat$tested==1,]

rbind(
  tibble(
    sample="full sample", 
    mean=round(mean(dat$bmi),2), 
    ci_lower=round(mean(dat$bmi) - 1.96*sqrt(var(dat$bmi)/nrow(dat)),2),
    ci_upper=round(mean(dat$bmi) + 1.96*sqrt(var(dat$bmi)/nrow(dat)
    ),2)
  ),
  tibble(
    sample="sub-sample", 
    mean=round(mean(dattest$bmi),2), 
    ci_lower=round(mean(dattest$bmi) - 1.96*sqrt(var(dattest$bmi)/nrow(dattest)),2),
    ci_upper=round(mean(dattest$bmi) + 1.96*sqrt(var(dattest$bmi)/nrow(dattest)),2)
  )
) %>% kable
```

### @Zhao2019

Suppose we only measure BMI in the tested sub-sample and not the full UK Biobank sample. Our probability weights are likely to be misspecified if we estimate them using only age and sex as covariates. The sensitivity parameter in @Zhao2019 is the largest amount that the estimated weights differ from the true weights on the odds ratio scale. Given this parameter, this method provides an interval of possible values for means and average treatment effects.

We begin by calculating our misspecified weights using only BMI and sex. For illustration, we calculate the true sensitivity parameter.

```{r zhao}
prs_mis <- as.vector(fitted.values(glm(tested ~ sex + age + age*sex, data=dat, family="binomial")))
prs_or <- (prs2/(1-prs2))/(prs_mis/(1-prs_mis))
Gamma <- max(1/min(prs_or),max(prs_or))
print(paste("True sensitivity parameter: ",round(Gamma,2),sep=""))

sens1 <- bootsens::extrema.md(A=dat$tested, Y=dat$bmi, X <- as.matrix(dat[,!(names(dat) %in% c("tested"))]), gamma=log(Gamma), estimand="all")

sens1_ci <- bootsens::bootsens.md(A=dat$tested, Y=dat$bmi, X <- as.matrix(dat[,!(names(dat) %in% c("tested"))]), gamma=log(Gamma), estimand="all", parallel=F, B=500)

out1 <- list(beta_min=sens1[1], beta_max=sens1[2], beta_lower=sens1_ci[1], beta_upper=sens1_ci[2])

tibble(
    method="Zhao et al (2019)", 
    lower_ci=round(out1$beta_lower,2), 
    lower_bound=round(out1$beta_min,2),
    upper_bound=round(out1$beta_max,2),
    upper_ci=round(out1$beta_upper,2)
) %>% kable
```

The resulting bounds, while quite wide, contain average BMI in the full UK Biobank population. One reason these bounds are so wide is because this method is non-parametric, that is, it does not restrict the way in the true weights may differ from the estimated weights. The trade-off is between precision (i.e. a tight bound) and accuracy (i.e. a bound that contains average BMI in the full UK Biobank population).

### @Tudball2020

Suppose we only observe the tested sub-sample. Estimating probability weights is no longer possible since we do not observe anyone who has not been tested. The two sensitivity parameters in @Tudball2020 are the smallest and largest probabilities of being tested (e.g. 1\% and 90\%). We then select a model for the probability weights as before and this method places bounds on the possible values of average BMI in the full UK Biobank population. 

An advantage of this method is that we can place additional constraints on the bounds. For example, suppose we know the unconditional likelihood of being tested and the average age of UK Biobank participants. We can include these as constraints to ensure that the resulting bounds are consistent with these two population values. 

A non-parametric version of this method also exists but here we present the parametric version using a logistic model for the weights. As discussed above, the parametric version results in tighter bounds but carries the risk of misspecification of the weights.

We first define the numerator and denominator of estimator. Since this is a mean, the numerator is the BMI variable and the denominator is a vector of ones. We also format the variables we wish to use in our probability weights. We select age, sex, BMI and their interactions.

```{r tudball_data}
fT <- dattest$bmi
gT <- rep(1,NROW(dattest))

D <- as.data.frame(dattest[,c('age','sex','bmi')])
D$agexsex <- D$age*D$sex
D$agexbmi <- D$age*D$bmi
D$sexxbmi <- D$sex*D$bmi
D$agexsexxbmi <- D$age*D$sex*D$bmi
```

We then define our lower bound (a) and upper bound (b) for the probability of being tested. For illustration, we use the true values estimated from the linear model with interactions.

```{r tudball_bound}
a <- min(prs2)
b <- max(prs2)
```

We also select our constraints and other options for the solver.

```{r}
mycons <- list(
  list('resp', mean(dat$tested)),
  list('covmean', D$sex, mean(dat$sex))
)

nlopts <- list("xtol_rel"=1e-8, "ftol_rel"=1e-10, "maxeval"=-1)
opts <- list("alpha2"=0.025, "grid_bound"=4, "grid_fine"=8, "tol"=1e-4, "maxeval"=3e2, "newton_step"=0.1)
```

We are now ready to generate our bounds for average BMI. To do this we will use the `find_bound` function that is defined in the main repository (https://github.com/MRCIEU/ukbb-covid-collider).

```{r tudball_options}
source("solve.R")
```

```{r tudball_calculate}
sens2 <- find_bound(a,b,fT,gT,D,constraints=mycons,opts,nlopts)

sens2_upper <- sens2$beta_max + qnorm(0.9875)*sens2$se_max
sens2_lower <- sens2$beta_min - qnorm(0.9875)*sens2$se_min

out2 <- list(beta_min=sens2$beta_min, beta_max=sens2$beta_max, beta_lower=sens2_lower, beta_upper=sens2_upper)

tibble(
  method="Tudball et al (2020)", 
  lower_ci=round(out2$beta_lower,2), 
  lower_bound=round(out2$beta_min,2),
  upper_bound=round(out2$beta_max,2),
  upper_ci=round(out2$beta_upper,2)
  ) %>% kable
```

As we can see, this method produces an informative bound for average BMI in the full sample.

## Summary

Here we have shown that inverse probability weighting is able to recover sample-wide associations between BMI, age and sex within the non-random covid-tested subset. We also show that if the data is non-nested, in that a model cannot be fitted to predict participation, then summary data from a reference population can be used to generate adjusted estimates within the selected sample.

In general practice, there are a few things to further consider. First, we only used BMI, age and sex to develop weights here. In principle one would actually use as many variables as is necessary (even if they are not the exact variables being analysed) to build the model for sample selection. Second, there is likely to be a change in selection pressure over time, so it is important to use weights that are relevant to the timings of the variables being recorded. Third, in the UK Biobank the reality is slightly more complicated than has been presented here, in that the samples in the overall UK Biobank are themselves non-random (and not representative of the general population) [@Munafo2018-fv].

## References